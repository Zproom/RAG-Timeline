{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43de859-9793-423b-82db-9de09182cc49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7520324a-b451-429e-8aa3-92f38a026093",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46d20d12-d42e-4927-91e2-515eeda00d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers\n",
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4857e506-4a7d-47af-aa70-e76ee1274119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "from enum import Enum\n",
    "from typing import Any\n",
    "from uuid import uuid4\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Third-party libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import newspaper as news\n",
    "from tqdm.notebook  import tqdm\n",
    "from newspaper.mthreading import fetch_news\n",
    "from gdeltdoc import GdeltDoc, Filters\n",
    "from spacy.lang.en import English\n",
    "from qdrant_client import QdrantClient\n",
    "from huggingface_hub import HfFolder, whoami\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct, QueryResponse\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e34919-76d7-4e23-9d23-5040f805e3cd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Init Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2851c793-fb06-4e2d-805e-4c480efcb6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f43965-8f2f-4adb-a769-2affa01022a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Login to hugging face to gain access to models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7965d5fc-0c11-4fd8-ba78-b91b4c02c7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hf_login():\n",
    "    \"\"\"\n",
    "    Helper method to log into hugging face to gain access to those models\n",
    "    \n",
    "    **NOTE**: exception will be raised if users hugging face access token is not \n",
    "              stored in the environment variable: HF_TOKEN\n",
    "    \"\"\"\n",
    "    HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "    if HF_TOKEN is None:\n",
    "        raise ValueError(\"CANNOT FIND HUGGING FACE ACCESS TOKEN\")\n",
    "\n",
    "    HfFolder.save_token(HF_TOKEN)\n",
    "\n",
    "    if logger.isEnabledFor(logging.DEBUG):\n",
    "        user = whoami()\n",
    "        logger.debug(f\"Logged into hugging face as: {user['fullname']} - {user['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ee69ae2-f066-4448-8c01-19db8526ce1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 02:04:04,974 - DEBUG - Logged into hugging face as: jacob - hundredcrane120\n"
     ]
    }
   ],
   "source": [
    "hf_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f56e52-b0c0-49d6-943e-9ddaf74f8e32",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Check CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f98ad032-384e-4f1d-a433-0dc6a1884033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 02:04:04,985 - DEBUG - CUDA detected. Device: NVIDIA GeForce RTX 5070 Ti\n"
     ]
    }
   ],
   "source": [
    "def check_cuda_device():\n",
    "    \"\"\"Helper method to print the connected cuda device\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        logger.debug(f\"CUDA detected. Device: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "    else:\n",
    "        logger.debug(\"CUDA not detected. Embedding model will remain on CPU\")\n",
    "\n",
    "check_cuda_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6217592-86ba-4e61-ad72-66f69fa2b0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 02:04:04,987 - DEBUG - Total memory:     17094.48 MB\n",
      "2025-08-04 02:04:04,987 - DEBUG - Reserved memory:  0.00 MB\n",
      "2025-08-04 02:04:04,987 - DEBUG - Allocated memory: 0.00 MB\n",
      "2025-08-04 02:04:04,987 - DEBUG - Free within reserved: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "def check_cuda_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_id = torch.cuda.current_device()\n",
    "        total = torch.cuda.get_device_properties(gpu_id).total_memory\n",
    "        reserved = torch.cuda.memory_reserved(gpu_id)\n",
    "        allocated = torch.cuda.memory_allocated(gpu_id)\n",
    "        free = reserved - allocated\n",
    "    \n",
    "        logger.debug(f\"Total memory:     {total / 1e6:.2f} MB\")\n",
    "        logger.debug(f\"Reserved memory:  {reserved / 1e6:.2f} MB\")\n",
    "        logger.debug(f\"Allocated memory: {allocated / 1e6:.2f} MB\")\n",
    "        logger.debug(f\"Free within reserved: {free / 1e6:.2f} MB\")\n",
    "    else:\n",
    "        logger.debug(\"No CUDA device available.\")\n",
    "\n",
    "check_cuda_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced24f08-88d1-44b5-aef6-fd142bfcae8b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Initialize Qdrant local server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a8bf9e6-ffb0-496c-aca8-9d6d41b4c11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of embedding from using the model: all-MiniLM-L6-v2\n",
    "EMBEDDING_LENGTH = 384\n",
    "COLLECTION_NAME = \"News_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93be0d6b-d8fa-485f-ad81-6c8b58b82c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VECTOR_DB.delete_collection(collection_name=COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3635c315-9bcc-4c4e-9ad6-596f6d7fad1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_DB = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "if not VECTOR_DB.collection_exists(COLLECTION_NAME):\n",
    "    VECTOR_DB.create_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        vectors_config=VectorParams(size=EMBEDDING_LENGTH, distance=Distance.COSINE)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8ba8f6-0a4a-483c-a068-57ca09b94a98",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Initialize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a7c92fd-2b57-438a-b196-d812d7507861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reranking_model(\n",
    "    model_name: str = \"cross-encoder/ms-marco-TinyBERT-L-2-v2\"\n",
    ") -> CrossEncoder:\n",
    "    \"\"\"Helper method to create a reranker model and send it to cuda\"\"\"\n",
    "    logger.debug(\"Creating re-ranker model\")\n",
    "\n",
    "    model = CrossEncoder(model_name, device=\"cpu\")\n",
    "\n",
    "    # if torch.cuda.is_available():\n",
    "    #     model = model.to(\"cuda\")\n",
    "    #     logger.debug(f\"CUDA detected. Embedding model moved to {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "    # else:\n",
    "    #     logger.debug(\"CUDA not detected. Embedding model will remain on CPU\")\n",
    "\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08000ab2-76eb-4b3f-8429-bba82c45812e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_model(model_name: str = \"all-MiniLM-L6-v2\") -> SentenceTransformer:\n",
    "    \"\"\"Helper method to create the model and send it to cuda\"\"\"\n",
    "    logger.debug(\"Creating embedding model\")\n",
    "    \n",
    "    model = SentenceTransformer(model_name_or_path=\"all-MiniLM-L6-v2\", device=\"cpu\")\n",
    "\n",
    "    # if torch.cuda.is_available():\n",
    "    #     model = model.to(\"cuda\")\n",
    "    #     logger.debug(f\"CUDA detected. Reranker model moved to {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "    # else:\n",
    "    #     logger.debug(\"CUDA not detected. Reranker model will remain on CPU\")\n",
    "\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fd6b139-bb62-4567-b1fe-47e6f08c4e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LLM(\n",
    "    model_name: str = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    ") -> tuple[AutoTokenizer, AutoModelForCausalLM]:\n",
    "    \"\"\"Helper method to create the LLM for generation and the associated tokenizer + send it to cuda\"\"\"\n",
    "    logger.debug(\"Creating LLM model\")\n",
    "\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
    "    \n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "    \n",
    "    config = {\n",
    "        \"pretrained_model_name_or_path\": model_name,        \n",
    "        \"torch_dtype\": torch.float16,\n",
    "        \"device_map\": \"auto\",\n",
    "        \"quantization_config\": quant_config\n",
    "    }\n",
    "    \n",
    "    # check if flash_attention_2 can be used\n",
    "    if is_flash_attn_2_available() and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
    "        logger.debug(f\"Using flash_attention_2\")\n",
    "        config[\"attn_implementation\"] = \"flash_attention_2\"\n",
    "    \n",
    "    llm_model = AutoModelForCausalLM.from_pretrained(**config)\n",
    "\n",
    "    return tokenizer, llm_model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68209634-357b-4da8-80dc-168cc5be4b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 02:04:19,755 - DEBUG - Creating embedding model\n",
      "2025-08-04 02:04:21,219 - DEBUG - Creating re-ranker model\n",
      "2025-08-04 02:04:21,843 - DEBUG - Creating LLM model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab591b9bb29244699f619d9405a7460e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 02:04:31,407 - DEBUG - All models created\n",
      "2025-08-04 02:04:31,413 - DEBUG - Total memory:     17094.48 MB\n",
      "2025-08-04 02:04:31,413 - DEBUG - Reserved memory:  7228.88 MB\n",
      "2025-08-04 02:04:31,413 - DEBUG - Allocated memory: 4141.95 MB\n",
      "2025-08-04 02:04:31,413 - DEBUG - Free within reserved: 3086.93 MB\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_MODEL = create_embedding_model()\n",
    "RERANKER_MODEL = create_reranking_model()\n",
    "TOKENIZER, LLM_MODEL = create_LLM()\n",
    "\n",
    "logger.debug(\"All models created\")\n",
    "check_cuda_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fcd4805-ae87-4760-91f4-4c521bab22bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = EMBEDDING_MODEL.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83b16534-7a90-4898-b29f-bd6eec6cb93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = RERANKER_MODEL.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8880d15a-54f9-43d5-a048-a62d11351123",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 02:05:44,457 - DEBUG - Total memory:     17094.48 MB\n",
      "2025-08-04 02:05:44,457 - DEBUG - Reserved memory:  7245.66 MB\n",
      "2025-08-04 02:05:44,457 - DEBUG - Allocated memory: 4250.36 MB\n",
      "2025-08-04 02:05:44,459 - DEBUG - Free within reserved: 2995.30 MB\n"
     ]
    }
   ],
   "source": [
    "check_cuda_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b31a041-8598-45ae-8c71-798d5443da4d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Extract data from GDelt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc34884a-55ae-41bf-a8c9-fd2fdbeaa13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_tqdm(iterable, desc=None, ignore: bool = False):\n",
    "    \"\"\"Only use tqdm progress bar while in debugging\"\"\"\n",
    "    if not ignore and logger.isEnabledFor(logging.DEBUG):\n",
    "        return tqdm(iterable, desc=desc)\n",
    "\n",
    "    return iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e903a91a-1c79-476d-9aca-e38e356bcaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Source(Enum):\n",
    "    # CBC = \"cbc.ca\"\n",
    "    CNN = \"cnn.com\"\n",
    "    # AP = \"apnews.com\"    # wasn't working for some reason\n",
    "    BBC = \"bbc.co.uk\"\n",
    "    # Wired = \"wired.com\"\n",
    "    # Reuters = \"reuters.com\"\n",
    "    NyTimes = \"nytimes.com\"\n",
    "    Guardian = \"theguardian.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4ac50cf-a0db-49c7-8032-8fa460816022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_text(text: str) -> str:\n",
    "    \"\"\"Removes newline characters and leading/trailing whitespace\"\"\"\n",
    "    return text.replace(\"\\n\", \" \").strip()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d940936-fed6-4b39-92b9-94a30ac206fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gdelt_row_to_dict(gd_row: pd.Series) -> dict:\n",
    "    \"\"\"Helper method to convert the output of a gd.article_search row into a dict\"\"\"\n",
    "    return {\n",
    "        \"url\": gd_row[\"url\"],\n",
    "        \"title\": gd_row[\"title\"],\n",
    "        \"domain\": gd_row[\"domain\"],\n",
    "        \"country\": gd_row[\"sourcecountry\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00151388-2232-4f13-99e8-bae51e588351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_to_dict(art: news.Article) -> dict:\n",
    "    \"\"\"Helper method to convert a Article into a dict\"\"\"\n",
    "    return {\n",
    "        \"text\": format_text(art.text),\n",
    "        \"title\": art.title,\n",
    "        \"authors\": art.authors,\n",
    "        \"date\": art.publish_date,\n",
    "        \"source\": art.source_url,\n",
    "        \"url\": art.original_url\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a9f5cc9-0f9a-4a84-9ae3-982fde4fb395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gdelt_stories(\n",
    "    keywords: str | None = None,\n",
    "    theme: str | None = None,\n",
    "    sources: list[Source] | None = None,\n",
    "    start_date: datetime = datetime.now().date() - timedelta(days=7),\n",
    "    end_date: datetime = datetime.now().date(),\n",
    "    num_records: int = 5\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Retrieves metadata and urls for relevant stories via the GDelt API\n",
    "    \"\"\"\n",
    "    \n",
    "    if keywords and len(keywords) < 5:\n",
    "        raise ValueError(\"Keywords must be gte 5 characters or the Gdelt API errors.\")\n",
    "\n",
    "    sources = sources or [s for s in Source]        \n",
    "    sources = [s.value for s in sources]\n",
    "        \n",
    "    if len(sources) < 2:\n",
    "        raise ValueError(\"Number of sources must be gte 2 or the Gdelt API errors.\")\n",
    "\n",
    "    if start_date > datetime.now().date() or end_date > datetime.now().date():\n",
    "        raise ValueError(\"News cannot be in the future...\")\n",
    "\n",
    "    if start_date > end_date:\n",
    "        raise ValueError(\"How you gunna end before you start?!?!\")        \n",
    "     \n",
    "    sources = [s for s in sources]\n",
    "    \n",
    "    start_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "    end_str = end_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    if keywords and theme:\n",
    "        f = Filters(\n",
    "            start_date = start_str,\n",
    "            end_date = end_str,\n",
    "            num_records = num_records,\n",
    "            domain = sources,\n",
    "            country = [\"UK\", \"US\"],\n",
    "            language = \"eng\",\n",
    "            keyword = keywords,\n",
    "            theme=theme\n",
    "        )\n",
    "    elif keywords:\n",
    "        f = Filters(\n",
    "            start_date = start_str,\n",
    "            end_date = end_str,\n",
    "            num_records = num_records,\n",
    "            domain = sources,\n",
    "            country = [\"US\"],#, \"UK\"],\n",
    "            language = \"eng\",\n",
    "            keyword = keywords,\n",
    "        )\n",
    "    elif theme:\n",
    "        f = Filters(\n",
    "            start_date = start_str,\n",
    "            end_date = end_str,\n",
    "            num_records = num_records,\n",
    "            domain = sources,\n",
    "            country = [\"UK\", \"US\"],\n",
    "            language = \"eng\",\n",
    "            theme=theme\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"both theme and keywords cannot be empty\")\n",
    "        \n",
    "    gd = GdeltDoc()\n",
    "    search_results = gd.article_search(f)\n",
    "\n",
    "    results = []\n",
    "    for i, row in search_results.iterrows():\n",
    "        results.append(gdelt_row_to_dict(row))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18ce12a4-7e8f-4d68-a0e2-838e9afc7026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles(urls: list[str] | str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Queries newspaper4k for the article associated with the provided url\n",
    "\n",
    "    Note:   fetch_news is an alternative to mutlithread the retrieval process.\n",
    "            However, this does not include the text property sooo were stuck with this.\n",
    "    \"\"\"\n",
    "\n",
    "    urls = urls if isinstance(urls, list) else [urls]\n",
    "    results = []\n",
    "    view_titles = set()\n",
    "    for url in log_tqdm(urls, \"Collecting Stories\"):\n",
    "\n",
    "        # news.articles is an expensive operation so verify \n",
    "        # the current story hasn't already been retrieved\n",
    "        possible_title = url.rsplit('/', 1)[-1]\n",
    "        if possible_title in view_titles:\n",
    "            logger.debug(f\"Removed duplicate story: {possible_title}\")\n",
    "            continue\n",
    "        view_titles.add(possible_title)\n",
    "        \n",
    "        try:\n",
    "            art = news.article(url)         \n",
    "        except:\n",
    "            # in case an article doesn't exist (404 returned) or can't be accessed \n",
    "            continue\n",
    "        \n",
    "        results.append(article_to_dict(art))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48f9809c-1561-4227-9957-2396512918b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_news(\n",
    "    keywords: str | None = None,\n",
    "    theme: str | None = None,\n",
    "    sources: list[Source] | None = None,\n",
    "    start_date: datetime = datetime.now().date() - timedelta(days=7),\n",
    "    end_date: datetime = datetime.now().date(),\n",
    "    num_stories: int = 5,\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Retrieves stories related to the specified keywords\n",
    "\n",
    "    **NOTE**: theme must be a value from the available list here: \n",
    "    http://data.gdeltproject.org/api/v2/guides/LOOKUP-GKGTHEMES.TXT\n",
    "    \"\"\"\n",
    "\n",
    "    logger.debug(\"Gdelt: retrieval beginning\")\n",
    "    story_links = gdelt_stories(\n",
    "        keywords=keywords, \n",
    "        theme=theme,\n",
    "        sources=sources, \n",
    "        start_date=start_date, \n",
    "        end_date=end_date, \n",
    "        num_records=num_stories\n",
    "    )\n",
    "\n",
    "    logger.debug(\"newpaper4k: retrieval beginning\")\n",
    "    stories = get_articles(\n",
    "        urls=[story[\"url\"] for story in story_links]\n",
    "    )\n",
    "\n",
    "    return stories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ff85f2-b1cb-42d1-84e6-d58f9987476c",
   "metadata": {},
   "source": [
    "#### Example for retrieving stories:\n",
    "\n",
    "\\*\\*Note\\*\\*: retrieve_news returns a list of dicts in the format created by the article_to_dict function\n",
    "\n",
    "(i.e., keys=[title, text, domain, country, author, url, date])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6cffef02-c75c-4b3c-91ac-cbe3dba077da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stories = retrieve_news(theme=\"IMMIGRATION\", num_stories=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95ce9883-28cc-47fe-8548-ae6879eba743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stories[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f652b316-4555-4709-be11-a26e439ff13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [u[\"url\"] for u in stories]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a79ff7d-12c8-47fe-9091-7f7904ec9f63",
   "metadata": {},
   "source": [
    "Results from above:\n",
    "\n",
    "dict_keys(['text', 'title', 'authors', 'date', 'source', 'url'])\n",
    "\n",
    "['https://www.theguardian.com/us-news/2025/jul/29/states-sue-trump-administration-snap-recipients-data',\n",
    " 'https://www.bbc.co.uk/news/articles/clyjggjplyqo',\n",
    " 'https://www.theguardian.com/us-news/2025/jul/30/ice-hiring-incentives-signing-bonuses',\n",
    " 'https://www.cnn.com/2025/07/30/politics/immigration-employees-reader-callout',\n",
    " 'https://www.theguardian.com/us-news/2025/jul/28/trump-acknowledges-real-starvation-in-gaza-and-tells-israel-to-let-in-every-ounce-of-food',\n",
    " 'https://www.theguardian.com/us-news/2025/aug/01/judge-tps-temporary-protected-status-trump-deportation',\n",
    " 'https://www.theguardian.com/society/2025/jul/30/population-migration-england-wales-data',\n",
    " 'https://www.theguardian.com/world/2025/jul/30/mexico-sheinbaum-alligator-alcatraz-trump',\n",
    " 'https://www.theguardian.com/uk-news/2025/aug/01/social-media-ads-promoting-small-boat-crossings-uk-banned']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b75b88e-7c36-448e-97a9-8089a861291e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Test samples for creating article embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a8bc311-8190-4dae-bba1-f8de91e7a4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 02:05:58,933 - DEBUG - Gdelt: retrieval beginning\n",
      "2025-08-04 02:06:00,301 - DEBUG - newpaper4k: retrieval beginning\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "355288f7c6134a718ea763a1073e0048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting Stories:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 02:06:26,971 - DEBUG - Removed duplicate story: immigration-birthright-citizenship-us-dg\n",
      "2025-08-04 02:06:27,518 - DEBUG - Removed duplicate story: immigration-employees-reader-callout\n",
      "2025-08-04 02:06:29,175 - DEBUG - Removed duplicate story: ice-arrests-migrants-courthouse\n",
      "2025-08-04 02:06:31,470 - DEBUG - Removed duplicate story: deportations-backfiring-trump-analysis\n",
      "2025-08-04 02:06:36,111 - DEBUG - Removed duplicate story: guatemalan-migrant-deported-mexico-trump-administration-return\n",
      "2025-08-04 02:06:36,111 - DEBUG - Removed duplicate story: guatemalan-migrant-deported-mexico-trump-administration-return\n",
      "2025-08-04 02:06:37,298 - DEBUG - Removed duplicate story: sanctuary-immigration-policies-chicago-illinois-lawsuit-dismissed\n"
     ]
    }
   ],
   "source": [
    "test_stories = retrieve_news(\n",
    "    theme=\"IMMIGRATION\", \n",
    "    num_stories=100, \n",
    "    start_date= datetime.now().date() - timedelta(days=90),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bdf346-fc3a-49bb-890f-8e3698a4dda5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Chunk the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "532e9146-6d81-4ab7-aff5-10d36b3db887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_list_of_dicts(dict_list: list[dict]) -> list[dict]:\n",
    "    \"\"\"Helper method to deep copy a list of dicts (not-recursively)\"\"\"\n",
    "    logger.debug(\"Creating copy of target list of dicts\")\n",
    "    new_list = []\n",
    "    for item in dict_list:\n",
    "        new_list.append(item.copy())\n",
    "\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "baaf1e8c-6e92-4cfa-b8ad-b52ec0419c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentencize_stories(stories: list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Uses spacy to convert the block of text provided by newspaper4k into sentences\n",
    "\n",
    "    **Note**: This is an inplace operation\n",
    "    \"\"\"\n",
    "    nlp = English()\n",
    "    _ = nlp.add_pipe(\"sentencizer\")\n",
    "    \n",
    "    logger.debug(\"Breaking text into sentences\")\n",
    "    # convert to sentences and ensure dtype is a str not spacy specific typing\n",
    "    for story in log_tqdm(stories, \"Sentencizing\"):\n",
    "        story[\"sents\"] = list(nlp(story[\"text\"]).sents)\n",
    "        story[\"sents\"] = [str(s) for s in story[\"sents\"]]\n",
    "\n",
    "    return stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "684a90ab-7f82-4f10-8239-b2102d4adac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(input_list: list[Any], max_item_count: int) -> list[list[Any]]:\n",
    "    \"\"\"\n",
    "    Splits a list of strings into a seperate lists with specified maximum number of items\n",
    "    \"\"\"\n",
    "    return [input_list[i:i+max_item_count] for i in range(0, len(input_list), max_item_count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0433624d-f681-4ff1-8bfa-9c90895446f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_sentences(stories: list[dict], chunk_size: int = 10) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Breaks the list of sentences into sublists with a maximum item count of chunk_size\n",
    "    \n",
    "    **Note**: This is an inplace operation\n",
    "    \"\"\"\n",
    "    logger.debug(f\"Chunking sentences, chunk_size: {chunk_size}\")\n",
    "    \n",
    "    if \"sents\" not in stories[0].keys():\n",
    "        raise ValueError(\"Out of order operation: Cannot chunk sentences before they exist\")\n",
    "        \n",
    "    for story in log_tqdm(stories, \"Chunking\"):\n",
    "        story[\"sent_chunks\"] = split_list(story[\"sents\"], chunk_size)\n",
    "        \n",
    "    return stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c210858e-cb3d-4e6d-8c5e-b928a15132da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_length_metadata(stories: list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Collects minor metadata regarding the length of the article\n",
    "    \n",
    "    **Note**: This is an inplace operation\n",
    "    \"\"\"\n",
    "    logger.debug(\"Adding article length metadata\")\n",
    "    \n",
    "    if any(key not in stories[0] for key in [\"sents\", \"sent_chunks\"]):\n",
    "        raise ValueError(\"Out of order operation: Collecting metadata requires all sub-items to be populated\")\n",
    "        \n",
    "    for story in log_tqdm(stories, \"Collecting Metadata\"):\n",
    "        story[\"num_sents\"] = len(story[\"sents\"])\n",
    "        story[\"num_tokens\"] = len(story[\"text\"]) // 4\n",
    "        story[\"num_words\"] = len(story[\"text\"].split(\" \"))\n",
    "        story[\"num_chars\"] = len(story[\"text\"])\n",
    "        story[\"num_chunks\"] = len(story[\"sent_chunks\"])\n",
    "\n",
    "    return stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a4d7288-d60a-4c08-8540-9a53d934b7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_into_chunk_list(story_list: list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Seperates the chunks in the story dict into individual dicts and returns\n",
    "    a list with each of these chunks as their own item\n",
    "    \"\"\"\n",
    "    logger.debug(\"Creating list of chunks from sublist in article dict\")\n",
    "    \n",
    "    chunk_list = []\n",
    "    for item in log_tqdm(story_list, \"Seperating Chunks\"):\n",
    "        # print(item[\"sent_chunks\"])\n",
    "        for chunk in item[\"sent_chunks\"]:\n",
    "\n",
    "            # populate each chunk with articles original metadata\n",
    "            chunk_dict = {\n",
    "                \"title\": item[\"title\"],\n",
    "                \"authors\": item[\"authors\"],\n",
    "                \"date\": item[\"date\"],\n",
    "                \"source\": item[\"source\"],\n",
    "                \"url\": item[\"url\"],\n",
    "            }\n",
    "    \n",
    "            # rejoin chunk sentences and format to more natural text (i.e., format end of sentence text)\n",
    "            chunk_text = \"\".join(chunk).replace(\"  \", \" \").strip()\n",
    "            chunk_text = re.sub(r\"\\.([A-Z])\", r\". \\1\", chunk_text)    \n",
    "            chunk_dict[\"text\"] = chunk_text\n",
    "\n",
    "            # don't log here because it would get messy\n",
    "            chunk_list.append(chunk_dict)\n",
    "\n",
    "    return chunk_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0ba8d9-8d6c-4d3f-97f2-aba8fee1a642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_length_metadata(chunks: list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Collects metadata regarding the length of the chunks\n",
    "    \n",
    "    **Note**: This is an inplace operation\n",
    "    \"\"\"\n",
    "    logger.debug(\"Adding article length metadata\")\n",
    "    \n",
    "    for chunk in log_tqdm(chunks, \"Collecting Metadata\"):\n",
    "        chunk[\"num_tokens\"] = len(chunk[\"text\"]) // 4\n",
    "        chunk[\"num_words\"] = len(chunk[\"text\"].split(\" \"))\n",
    "        chunk[\"num_chars\"] = len(chunk[\"text\"])\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e40990d7-fe38-4f46-ae5c-945d9e13ae2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_articles(stories: list[dict], sentences_per_chunk: int = 10) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Converts the list of stories (stored as dicts) into a list chunks (also dicts)\n",
    "\n",
    "    sentences_per_chunks sets the number of sentences used in each chunk\n",
    "    \"\"\"\n",
    "    logger.debug(\"Chunking list of articles\")\n",
    "\n",
    "    # create chunk information within a copy of the stories list[dict]\n",
    "    story_copy = copy_list_of_dicts(stories)\n",
    "    story_copy = sentencize_stories(story_copy)\n",
    "    story_copy = chunk_sentences(story_copy)\n",
    "\n",
    "    # create a new chunk list[dict] from result\n",
    "    chunks = seperate_into_chunk_list(story_copy)\n",
    "    chunks = chunk_length_metadata(chunks)\n",
    "    \n",
    "    return chunks    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be4723d-91bf-4ab7-9d90-1cd61f6a8561",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Create embeddings from chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1cfe5a19-e518-4487-9282-597a5de80543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_embeddings(\n",
    "    chunk_list: list[dict], \n",
    "    embedding_model: SentenceTransformer\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Uses the embedded model to create embeddings from the text values in the provided chunks\n",
    "\n",
    "    **Note**: this operation is inplace\n",
    "    \"\"\"\n",
    "    logger.debug(\"Creating embeddings for chunks\")\n",
    "\n",
    "    chunk_texts = [chunk[\"text\"] for chunk in chunk_list]\n",
    "    chunk_embeddings = embedding_model.encode(\n",
    "        chunk_texts, \n",
    "        batch_size=32, \n",
    "        convert_to_tensor=True, \n",
    "        show_progress_bar=logger.isEnabledFor(logging.DEBUG)\n",
    "    )\n",
    "\n",
    "    for chunk, embedding in zip(chunk_list, chunk_embeddings):\n",
    "        chunk[\"embedding\"] = embedding\n",
    "\n",
    "    return chunk_list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0e4cd8d3-e9b4-4236-8171-2c57e6d56fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_and_embed(\n",
    "    articles_list: list[dict], \n",
    "    embedding_model: SentenceTransformer | None = None,\n",
    ") -> list[dict]:\n",
    "\n",
    "    # initialize a model if one was not provided\n",
    "    embedding_model = embedding_model or EMBEDDING_MODEL\n",
    "\n",
    "    # create chunks and add embeddings\n",
    "    chunks = chunk_articles(articles_list)\n",
    "    chunks = add_embeddings(chunks, embedding_model=embedding_model)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e8bbd2-5153-4cf4-9a09-802717362910",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Storage in QDrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6050b31c-5391-467f-bc01-173ad19b1268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_chunk_list_to_db(chunk_list: dict):\n",
    "    \"\"\"Adds a single chunk to the vector database\"\"\"\n",
    "    logger.debug(f\"Adding chunks to vector DB, collection: {COLLECTION_NAME}\")\n",
    "\n",
    "    points = [\n",
    "        PointStruct(\n",
    "            id = str(uuid4()),\n",
    "            vector = chunk[\"embedding\"],\n",
    "            payload = {\n",
    "                \"source\": chunk[\"source\"],\n",
    "                \"date\": chunk[\"date\"],\n",
    "                \"url\": chunk[\"url\"],\n",
    "                \"title\": chunk[\"title\"],\n",
    "                \"authors\": chunk[\"authors\"],\n",
    "                \"text\": chunk[\"text\"],\n",
    "            }\n",
    "        )\n",
    "        for chunk in chunk_list\n",
    "    ]\n",
    "\n",
    "    VECTOR_DB.upsert(collection_name=COLLECTION_NAME, points=points)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bd11c64a-fc6d-46d8-9ce4-35cb0c0cd6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks = chunk_and_embed(test_stories)\n",
    "# add_chunk_list_to_db(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2a6a75-c764-430d-94e2-06ad9dbf5a08",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Qdrant Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7adc1a80-c31f-46d7-9c22-d84d5e125cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_response_to_dict(\n",
    "    response: QueryResponse, \n",
    "    embedding_model: SentenceTransformer\n",
    ") -> list[dict]:\n",
    "    \"\"\"Converts the output of the qdrant similarity search to a list of dicts\"\"\"\n",
    "    logger.debug(\"Converting query results into a dictionary\")\n",
    "\n",
    "    results = []\n",
    "    for point in response.points:\n",
    "        new_dict = point.payload.copy()\n",
    "        new_dict[\"score\"] = point.score\n",
    "        new_dict[\"vector\"] = point.vector\n",
    "        results.append(new_dict)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a85d3d98-228b-4af2-a221-7e22fd129823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_db(\n",
    "    query_str: str, embedding_model: SentenceTransformer, num_retrieve: int = 5\n",
    ") -> QueryResponse:\n",
    "    \"\"\"Method for searching for qdrant db for similar chunks to provided quote\"\"\"\n",
    "    \n",
    "    # leave embeddings in default type. Tensor isn't accepted and converting to np increases runtime.\n",
    "    query_embeddings = embedding_model.encode(query_str)\n",
    "    results = VECTOR_DB.query_points(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        query=query_embeddings,\n",
    "        limit=num_retrieve,\n",
    "        with_payload=True,\n",
    "        with_vectors=True\n",
    "    )\n",
    "    \n",
    "    return query_response_to_dict(results, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "be72b3a3-1fa8-469a-8d6b-89e32b7ecbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_response(\n",
    "    query_str: str,\n",
    "    resp_list: list[dict],\n",
    "    reranker_model: CrossEncoder\n",
    ") -> list[tuple[float, dict]]:\n",
    "    \"\"\"\n",
    "    Takes the output from our search in the vector database and reranks based on crossencoder similarity\n",
    "    \"\"\"\n",
    "    logger.debug(f\"Reranking responses [num_query_results: {len(resp_list)}]\")\n",
    "\n",
    "    # extract the original chunk text from the payloads\n",
    "    response_texts = [item[\"text\"] for item in resp_list]\n",
    "\n",
    "    # pass query string with text into reranker\n",
    "    text_pairs = [(query_str, text) for text in response_texts]\n",
    "    new_scores = reranker_model.predict(\n",
    "        text_pairs, \n",
    "        batch_size=32, \n",
    "        show_progress_bar=logger.isEnabledFor(logging.DEBUG),\n",
    "    )\n",
    "\n",
    "    ordered_results = sorted(zip(new_scores.astype(float), resp_list), key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    return ordered_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ec14d79f-89c4-43a5-801d-25c1589d5952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_similar(\n",
    "    query_str: str, \n",
    "    embedding_model: SentenceTransformer | None = None,\n",
    "    reranker_model: CrossEncoder | None = None,\n",
    "    num_retrieve: int = 5,\n",
    ") -> list[tuple[float, dict]]:\n",
    "    \"\"\"Retrieves the most similar items in the db to the query\"\"\"\n",
    "    logger.debug(f\"Querying VECTOR_DB for chunks similar to: {query_str} and rank\")\n",
    "\n",
    "    # if no embedding model sets to globally initialize one\n",
    "    embedding_model = embedding_model or EMBEDDING_MODEL\n",
    "    reranker_model = reranker_model or RERANKER_MODEL\n",
    "\n",
    "    # search for 3 times the requested amount and filter via the re-ranker\n",
    "    search_results = query_db(query_str, EMBEDDING_MODEL, num_retrieve * 3)\n",
    "    rerank = rerank_response(query_str, search_results, RERANKER_MODEL)\n",
    "    \n",
    "    return rerank[:num_retrieve]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "23ab223d-be7f-4a99-96a2-20ee0de08da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = retrieve_similar(\"Trump immigration\", num_retrieve=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9c63442e-8a04-4ada-8424-532880ab6479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x[0][1][\"title\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eaf3c0-ad13-473f-9e5b-66ce82bbea79",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Configuring a local LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cfea4339-a46c-4b54-a906-0242bbbed524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context_str(resources: list[tuple[float, dict]]) -> str:\n",
    "    \"\"\"converts the list output from retrieve_similar into a str\"\"\"\n",
    "\n",
    "    context = \"\"\n",
    "    for i, (_, chunk) in enumerate(resources):\n",
    "        context += f\"article {i}: {chunk['title']}\\n\"\n",
    "        context += f\"text: {chunk['text']}\\n\\n\"\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5a11cf83-fc31-4b02-a65b-46363b2a3664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_factory(\n",
    "    query: str,\n",
    "    resources: list[tuple[float, dict]],\n",
    "    tokenizer: AutoTokenizer | None = None,\n",
    ") -> str:\n",
    "    \"\"\"Generates the prompt for the RAG based on a template with the query + resource context\"\"\"\n",
    "    logger.debug(f\"Generating prompt\")\n",
    "\n",
    "    tokenizer = tokenizer or TOKENIZER\n",
    "\n",
    "    prompt_str = \"You are an expert news summarizer. \" \\\n",
    "                + \"Using the clips from news articles provided below\" \\\n",
    "                + \"generate a clear and concise summary of the query.\\n\\n\"\"\"\n",
    "    prompt_str += create_context_str(resources)\n",
    "    prompt_str += f\"\\nUser Query:\\n{query}\"\n",
    "    prompt_str += \"\\nAnswer\"\n",
    "\n",
    "    template = [{\"role\": \"user\", \"content\": prompt_str}]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        template, \n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    return prompt   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9c26b6b2-2dfe-4c48-ab7c-fb2f315f56eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    prompt: str,\n",
    "    temperature: float = 0.7,\n",
    "    max_new_tokens: int = 256,\n",
    "    format_answer_text: bool = True,\n",
    "    return_answer_only: bool = True,\n",
    "    tokenizer: AutoTokenizer | None = None,\n",
    "    llm_model: AutoModelForCausalLM | None = None\n",
    ") -> str:\n",
    "    \"\"\"Method for passing the prompt into the LLM_Model with context\"\"\"\n",
    "    logger.debug(\"Passing prompt to LLM for generation...\")\n",
    "    \n",
    "    # generate - tokens\n",
    "    input_tokens = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # generate output\n",
    "    output = llm_model.generate(\n",
    "        **input_tokens,\n",
    "        temperature=temperature,\n",
    "        do_sample=True,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        pad_token_id=tokenizer.eos_token_id # just to get rid of a warning msg\n",
    "    )\n",
    "    logger.debug(\"LLM generation complete.\")\n",
    "\n",
    "    return tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6efc3cde-e1d7-4eea-a5e5-e2811f07f4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(\n",
    "    query_str: str,\n",
    "    temperature: float = 0.7,\n",
    "    max_new_tokens: int = 256,\n",
    "    format_answer_text: bool = True,\n",
    "    return_answer_only: bool = True,\n",
    "    tokenizer: AutoTokenizer | None = None,\n",
    "    llm_model: AutoModelForCausalLM | None = None,\n",
    ") -> str | tuple[str, list[tuple[float, dict]]]:\n",
    "    \"\"\"Full method for querying our RAG\"\"\"\n",
    "    logger.debug(f\"Asking RAG, query: {query_str}\")\n",
    "    \n",
    "    tokenizer = tokenizer or TOKENIZER\n",
    "    llm_model = llm_model or LLM_MODEL\n",
    "\n",
    "    # retrieval\n",
    "    resources = retrieve_similar(input_text, 5)\n",
    "\n",
    "    # augment\n",
    "    prompt = prompt_factory(query_str, resources, tokenizer)\n",
    "\n",
    "    # generate llm response\n",
    "    output_text = generate(\n",
    "        prompt,\n",
    "        temperature,\n",
    "        max_new_tokens,\n",
    "        format_answer_text,\n",
    "        return_answer_only,\n",
    "        tokenizer,\n",
    "        llm_model\n",
    "    )\n",
    "    \n",
    "    # formatting\n",
    "    if format_answer_text:\n",
    "        output_text = output_text.replace(prompt, \"\").replace(\"<s> \", \"\").replace(\"</s>\", \"\")\n",
    "\n",
    "    if return_answer_only:\n",
    "        return output_text\n",
    "\n",
    "    return output_text, resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7491acf3-2791-4ac5-96b9-a2e1b83c06c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"what is going on with Trumps immigration policies?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "06080350-c451-46b3-af5a-bdbe4b21902d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 03:14:29,376 - DEBUG - Asking RAG, query: what is going on with Trumps immigration policies?\n",
      "2025-08-04 03:14:29,376 - DEBUG - Querying VECTOR_DB for chunks similar to: what is going on with Trumps immigration policies? and rank\n",
      "C:\\Users\\jacob\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "2025-08-04 03:14:29,393 - DEBUG - Converting query results into a dictionary\n",
      "2025-08-04 03:14:29,393 - DEBUG - Reranking responses [num_query_results: 15]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08bd03277aa04783ab9541cc25941dd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 03:14:29,414 - DEBUG - Generating prompt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"President Donald Trump's immigration policies in his second term have focused on enacting a crackdown, with increased arrests and deportations. The administration is resurrecting old policies, aiming to regulate the flow of immigrants and address national security concerns. This approach has been criticized by many Americans, with a recent CNN poll showing that about 52% believe Trump has gone too far in deporting undocumented immigrants, and 57% do not believe the federal government is being careful in following the law while carrying out deportations. The administration has also faced backlash for its aggressive immigration enforcement actions, with concerns raised about due process rights and constitutional violations. Despite this, the administration has continued to prioritize immigration as a key issue, but recent polling shows that Americans disapprove of Trump's handling of immigration by a wide margin.\""
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "21b77154-c675-45dd-9506-dd958ae38707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 03:18:04,753 - DEBUG - Total memory:     17094.48 MB\n",
      "2025-08-04 03:18:04,754 - DEBUG - Reserved memory:  7249.85 MB\n",
      "2025-08-04 03:18:04,754 - DEBUG - Allocated memory: 4258.88 MB\n",
      "2025-08-04 03:18:04,754 - DEBUG - Free within reserved: 2990.97 MB\n"
     ]
    }
   ],
   "source": [
    "check_cuda_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a7fc74-7a0c-4f67-bf90-b60150667971",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "- Local Retrieval Augmented Generation (RAG) from Scratch (step by step tutorial) by Daniel Bourke\n",
    "\n",
    "link: https://www.youtube.com/watch?v=qN_2fnOPY-M"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
