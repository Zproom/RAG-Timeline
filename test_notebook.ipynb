{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7520324a-b451-429e-8aa3-92f38a026093",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "4857e506-4a7d-47af-aa70-e76ee1274119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import re\n",
    "import logging\n",
    "from enum import Enum\n",
    "from typing import Any\n",
    "from uuid import uuid4\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Third-party libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import newspaper as news\n",
    "from tqdm.notebook  import tqdm\n",
    "from newspaper.mthreading import fetch_news\n",
    "from gdeltdoc import GdeltDoc, Filters\n",
    "from spacy.lang.en import English\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct, QueryResponse\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb95691f-441a-4f3e-88b3-e046a210fec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f56e52-b0c0-49d6-943e-9ddaf74f8e32",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Check CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f98ad032-384e-4f1d-a433-0dc6a1884033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 18:58:12,152 - DEBUG - CUDA detected. Device: NVIDIA GeForce RTX 5070 Ti\n"
     ]
    }
   ],
   "source": [
    "def check_cuda_device():\n",
    "    \"\"\"Helper method to print the connected cuda device\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        logger.debug(f\"CUDA detected. Device: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "    else:\n",
    "        logger.debug(\"CUDA not detected. Embedding model will remain on CPU\")\n",
    "\n",
    "check_cuda_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c6217592-86ba-4e61-ad72-66f69fa2b0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 18:58:14,162 - DEBUG - Total memory:     17094.48 MB\n",
      "2025-08-03 18:58:14,162 - DEBUG - Reserved memory:  369.10 MB\n",
      "2025-08-03 18:58:14,162 - DEBUG - Allocated memory: 100.64 MB\n",
      "2025-08-03 18:58:14,167 - DEBUG - Free within reserved: 268.46 MB\n"
     ]
    }
   ],
   "source": [
    "def check_cuda_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_id = torch.cuda.current_device()\n",
    "        total = torch.cuda.get_device_properties(gpu_id).total_memory\n",
    "        reserved = torch.cuda.memory_reserved(gpu_id)\n",
    "        allocated = torch.cuda.memory_allocated(gpu_id)\n",
    "        free = reserved - allocated\n",
    "    \n",
    "        logger.debug(f\"Total memory:     {total / 1e6:.2f} MB\")\n",
    "        logger.debug(f\"Reserved memory:  {reserved / 1e6:.2f} MB\")\n",
    "        logger.debug(f\"Allocated memory: {allocated / 1e6:.2f} MB\")\n",
    "        logger.debug(f\"Free within reserved: {free / 1e6:.2f} MB\")\n",
    "    else:\n",
    "        logger.debug(\"No CUDA device available.\")\n",
    "\n",
    "check_cuda_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced24f08-88d1-44b5-aef6-fd142bfcae8b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Initialize Qdrant local server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2a8bf9e6-ffb0-496c-aca8-9d6d41b4c11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of embedding from using the model: all-MiniLM-L6-v2\n",
    "EMBEDDING_LENGTH = 384\n",
    "COLLECTION_NAME = \"News_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "93be0d6b-d8fa-485f-ad81-6c8b58b82c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VECTOR_DB.delete_collection(collection_name=COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3635c315-9bcc-4c4e-9ad6-596f6d7fad1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_DB = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "if not VECTOR_DB.collection_exists(COLLECTION_NAME):\n",
    "    VECTOR_DB.create_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        vectors_config=VectorParams(size=EMBEDDING_LENGTH, distance=Distance.COSINE)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8ba8f6-0a4a-483c-a068-57ca09b94a98",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Initialize Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "3a7c92fd-2b57-438a-b196-d812d7507861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reranking_model(\n",
    "    model_name: str = \"cross-encoder/ms-marco-TinyBERT-L-2-v2\"\n",
    ") -> CrossEncoder:\n",
    "    \"\"\"Helper method to create a reranker model and send it to cuda\"\"\"\n",
    "    logger.debug(\"Creating re-ranker model\")\n",
    "\n",
    "    model = CrossEncoder(model_name)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.to(\"cuda\")\n",
    "        logger.debug(f\"CUDA detected. Embedding model moved to {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "    else:\n",
    "        logger.debug(\"CUDA not detected. Embedding model will remain on CPU\")\n",
    "\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "08000ab2-76eb-4b3f-8429-bba82c45812e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_model(model_name: str = \"all-MiniLM-L6-v2\") -> SentenceTransformer:\n",
    "    \"\"\"Helper method to create the model and send it to cuda\"\"\"\n",
    "    logger.debug(\"Creating embedding model\")\n",
    "    \n",
    "    model = SentenceTransformer(model_name_or_path=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.to(\"cuda\")\n",
    "        logger.debug(f\"CUDA detected. Reranker model moved to {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "    else:\n",
    "        logger.debug(\"CUDA not detected. Reranker model will remain on CPU\")\n",
    "\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "68209634-357b-4da8-80dc-168cc5be4b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 23:35:24,389 - DEBUG - Creating embedding model\n",
      "2025-08-03 23:35:25,280 - DEBUG - CUDA detected. Embedding model moved to NVIDIA GeForce RTX 5070 Ti\n",
      "2025-08-03 23:35:25,280 - DEBUG - Creating re-ranker model\n",
      "2025-08-03 23:35:25,874 - DEBUG - CUDA detected. Embedding model moved to NVIDIA GeForce RTX 5070 Ti\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_MODEL = create_embedding_model()\n",
    "RERANKER_MODEL = create_reranking_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b31a041-8598-45ae-8c71-798d5443da4d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Extract data from GDelt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2851c793-fb06-4e2d-805e-4c480efcb6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc34884a-55ae-41bf-a8c9-fd2fdbeaa13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_tqdm(iterable, desc=None, ignore: bool = False):\n",
    "    \"\"\"Only use tqdm progress bar while in debugging\"\"\"\n",
    "    if not ignore and logger.isEnabledFor(logging.DEBUG):\n",
    "        return tqdm(iterable, desc=desc)\n",
    "\n",
    "    return iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e903a91a-1c79-476d-9aca-e38e356bcaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Source(Enum):\n",
    "    # CBC = \"cbc.ca\"\n",
    "    CNN = \"cnn.com\"\n",
    "    # AP = \"apnews.com\"    # wasn't working for some reason\n",
    "    BBC = \"bbc.co.uk\"\n",
    "    # Wired = \"wired.com\"\n",
    "    # Reuters = \"reuters.com\"\n",
    "    NyTimes = \"nytimes.com\"\n",
    "    Guardian = \"theguardian.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4ac50cf-a0db-49c7-8032-8fa460816022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_text(text: str) -> str:\n",
    "    \"\"\"Removes newline characters and leading/trailing whitespace\"\"\"\n",
    "    return text.replace(\"\\n\", \" \").strip()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d940936-fed6-4b39-92b9-94a30ac206fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gdelt_row_to_dict(gd_row: pd.Series) -> dict:\n",
    "    \"\"\"Helper method to convert the output of a gd.article_search row into a dict\"\"\"\n",
    "    return {\n",
    "        \"url\": gd_row[\"url\"],\n",
    "        \"title\": gd_row[\"title\"],\n",
    "        \"domain\": gd_row[\"domain\"],\n",
    "        \"country\": gd_row[\"sourcecountry\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00151388-2232-4f13-99e8-bae51e588351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_to_dict(art: news.Article) -> dict:\n",
    "    \"\"\"Helper method to convert a Article into a dict\"\"\"\n",
    "    return {\n",
    "        \"text\": format_text(art.text),\n",
    "        \"title\": art.title,\n",
    "        \"authors\": art.authors,\n",
    "        \"date\": art.publish_date,\n",
    "        \"source\": art.source_url,\n",
    "        \"url\": art.original_url\n",
    "    }    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a9f5cc9-0f9a-4a84-9ae3-982fde4fb395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gdelt_stories(\n",
    "    keywords: str | None = None,\n",
    "    theme: str | None = None,\n",
    "    sources: list[Source] | None = None,\n",
    "    start_date: datetime = datetime.now().date() - timedelta(days=7),\n",
    "    end_date: datetime = datetime.now().date(),\n",
    "    num_records: int = 5\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Retrieves metadata and urls for relevant stories via the GDelt API\n",
    "    \"\"\"\n",
    "    \n",
    "    if keywords and len(keywords) < 5:\n",
    "        raise ValueError(\"Keywords must be gte 5 characters or the Gdelt API errors.\")\n",
    "\n",
    "    sources = sources or [s for s in Source]        \n",
    "    sources = [s.value for s in sources]\n",
    "        \n",
    "    if len(sources) < 2:\n",
    "        raise ValueError(\"Number of sources must be gte 2 or the Gdelt API errors.\")\n",
    "\n",
    "    if start_date > datetime.now().date() or end_date > datetime.now().date():\n",
    "        raise ValueError(\"News cannot be in the future...\")\n",
    "\n",
    "    if start_date > end_date:\n",
    "        raise ValueError(\"How you gunna end before you start?!?!\")        \n",
    "     \n",
    "    sources = [s for s in sources]\n",
    "    \n",
    "    start_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "    end_str = end_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    if keywords and theme:\n",
    "        f = Filters(\n",
    "            start_date = start_str,\n",
    "            end_date = end_str,\n",
    "            num_records = num_records,\n",
    "            domain = sources,\n",
    "            country = [\"UK\", \"US\"],\n",
    "            language = \"eng\",\n",
    "            keyword = keywords,\n",
    "            theme=theme\n",
    "        )\n",
    "    elif keywords:\n",
    "        f = Filters(\n",
    "            start_date = start_str,\n",
    "            end_date = end_str,\n",
    "            num_records = num_records,\n",
    "            domain = sources,\n",
    "            country = [\"US\"],#, \"UK\"],\n",
    "            language = \"eng\",\n",
    "            keyword = keywords,\n",
    "        )\n",
    "    elif theme:\n",
    "        f = Filters(\n",
    "            start_date = start_str,\n",
    "            end_date = end_str,\n",
    "            num_records = num_records,\n",
    "            domain = sources,\n",
    "            country = [\"UK\", \"US\"],\n",
    "            language = \"eng\",\n",
    "            theme=theme\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"both theme and keywords cannot be empty\")\n",
    "        \n",
    "    gd = GdeltDoc()\n",
    "    search_results = gd.article_search(f)\n",
    "\n",
    "    results = []\n",
    "    for i, row in search_results.iterrows():\n",
    "        results.append(gdelt_row_to_dict(row))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18ce12a4-7e8f-4d68-a0e2-838e9afc7026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles(urls: list[str] | str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Queries newspaper4k for the article associated with the provided url\n",
    "\n",
    "    Note:   fetch_news is an alternative to mutlithread the retrieval process.\n",
    "            However, this does not include the text property sooo were stuck with this.\n",
    "    \"\"\"\n",
    "\n",
    "    urls = urls if isinstance(urls, list) else [urls]\n",
    "    results = []\n",
    "    view_titles = set()\n",
    "    for url in log_tqdm(urls, \"Collecting Stories\"):\n",
    "\n",
    "        # news.articles is an expensive operation so verify \n",
    "        # the current story hasn't already been retrieved\n",
    "        possible_title = url.rsplit('/', 1)[-1]\n",
    "        if possible_title in view_titles:\n",
    "            logger.debug(f\"Removed duplicate story: {possible_title}\")\n",
    "            continue\n",
    "        view_titles.add(possible_title)\n",
    "        \n",
    "        try:\n",
    "            art = news.article(url)         \n",
    "        except:\n",
    "            # in case an article doesn't exist (404 returned) or can't be accessed \n",
    "            continue\n",
    "        \n",
    "        results.append(article_to_dict(art))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48f9809c-1561-4227-9957-2396512918b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_news(\n",
    "    keywords: str | None = None,\n",
    "    theme: str | None = None,\n",
    "    sources: list[Source] | None = None,\n",
    "    start_date: datetime = datetime.now().date() - timedelta(days=7),\n",
    "    end_date: datetime = datetime.now().date(),\n",
    "    num_stories: int = 5,\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Retrieves stories related to the specified keywords\n",
    "\n",
    "    **NOTE**: theme must be a value from the available list here: \n",
    "    http://data.gdeltproject.org/api/v2/guides/LOOKUP-GKGTHEMES.TXT\n",
    "    \"\"\"\n",
    "\n",
    "    logger.debug(\"Gdelt: retrieval beginning\")\n",
    "    story_links = gdelt_stories(\n",
    "        keywords=keywords, \n",
    "        theme=theme,\n",
    "        sources=sources, \n",
    "        start_date=start_date, \n",
    "        end_date=end_date, \n",
    "        num_records=num_stories\n",
    "    )\n",
    "\n",
    "    logger.debug(\"newpaper4k: retrieval beginning\")\n",
    "    stories = get_articles(\n",
    "        urls=[story[\"url\"] for story in story_links]\n",
    "    )\n",
    "\n",
    "    return stories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ff85f2-b1cb-42d1-84e6-d58f9987476c",
   "metadata": {},
   "source": [
    "#### Example for retrieving stories:\n",
    "\n",
    "\\*\\*Note\\*\\*: retrieve_news returns a list of dicts in the format created by the article_to_dict function\n",
    "\n",
    "(i.e., keys=[title, text, domain, country, author, url, date])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cffef02-c75c-4b3c-91ac-cbe3dba077da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stories = retrieve_news(theme=\"IMMIGRATION\", num_stories=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95ce9883-28cc-47fe-8548-ae6879eba743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stories[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f652b316-4555-4709-be11-a26e439ff13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [u[\"url\"] for u in stories]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a79ff7d-12c8-47fe-9091-7f7904ec9f63",
   "metadata": {},
   "source": [
    "Results from above:\n",
    "\n",
    "dict_keys(['text', 'title', 'authors', 'date', 'source', 'url'])\n",
    "\n",
    "['https://www.theguardian.com/us-news/2025/jul/29/states-sue-trump-administration-snap-recipients-data',\n",
    " 'https://www.bbc.co.uk/news/articles/clyjggjplyqo',\n",
    " 'https://www.theguardian.com/us-news/2025/jul/30/ice-hiring-incentives-signing-bonuses',\n",
    " 'https://www.cnn.com/2025/07/30/politics/immigration-employees-reader-callout',\n",
    " 'https://www.theguardian.com/us-news/2025/jul/28/trump-acknowledges-real-starvation-in-gaza-and-tells-israel-to-let-in-every-ounce-of-food',\n",
    " 'https://www.theguardian.com/us-news/2025/aug/01/judge-tps-temporary-protected-status-trump-deportation',\n",
    " 'https://www.theguardian.com/society/2025/jul/30/population-migration-england-wales-data',\n",
    " 'https://www.theguardian.com/world/2025/jul/30/mexico-sheinbaum-alligator-alcatraz-trump',\n",
    " 'https://www.theguardian.com/uk-news/2025/aug/01/social-media-ads-promoting-small-boat-crossings-uk-banned']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b75b88e-7c36-448e-97a9-8089a861291e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Test samples for creating article embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a8bc311-8190-4dae-bba1-f8de91e7a4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 18:11:06,987 - DEBUG - Gdelt: retrieval beginning\n",
      "2025-08-03 18:11:07,860 - DEBUG - newpaper4k: retrieval beginning\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07a131f8190d46a792816844bf676f12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting Stories:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 18:11:42,428 - DEBUG - Removed duplicate story: immigration-birthright-citizenship-us-dg\n",
      "2025-08-03 18:11:42,936 - DEBUG - Removed duplicate story: immigration-employees-reader-callout\n",
      "2025-08-03 18:11:44,985 - DEBUG - Removed duplicate story: ice-arrests-migrants-courthouse\n",
      "2025-08-03 18:11:48,698 - DEBUG - Removed duplicate story: deportations-backfiring-trump-analysis\n",
      "2025-08-03 18:11:54,918 - DEBUG - Removed duplicate story: guatemalan-migrant-deported-mexico-trump-administration-return\n",
      "2025-08-03 18:11:54,918 - DEBUG - Removed duplicate story: guatemalan-migrant-deported-mexico-trump-administration-return\n",
      "2025-08-03 18:11:55,734 - DEBUG - Removed duplicate story: sanctuary-immigration-policies-chicago-illinois-lawsuit-dismissed\n"
     ]
    }
   ],
   "source": [
    "test_stories = retrieve_news(\n",
    "    theme=\"IMMIGRATION\", \n",
    "    num_stories=100, \n",
    "    start_date= datetime.now().date() - timedelta(days=90),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bdf346-fc3a-49bb-890f-8e3698a4dda5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Chunk the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "532e9146-6d81-4ab7-aff5-10d36b3db887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_list_of_dicts(dict_list: list[dict]) -> list[dict]:\n",
    "    \"\"\"Helper method to deep copy a list of dicts (not-recursively)\"\"\"\n",
    "    logger.debug(\"Creating copy of target list of dicts\")\n",
    "    new_list = []\n",
    "    for item in dict_list:\n",
    "        new_list.append(item.copy())\n",
    "\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "baaf1e8c-6e92-4cfa-b8ad-b52ec0419c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentencize_stories(stories: list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Uses spacy to convert the block of text provided by newspaper4k into sentences\n",
    "\n",
    "    **Note**: This is an inplace operation\n",
    "    \"\"\"\n",
    "    nlp = English()\n",
    "    _ = nlp.add_pipe(\"sentencizer\")\n",
    "    \n",
    "    logger.debug(\"Breaking text into sentences\")\n",
    "    # convert to sentences and ensure dtype is a str not spacy specific typing\n",
    "    for story in log_tqdm(stories, \"Sentencizing\"):\n",
    "        story[\"sents\"] = list(nlp(story[\"text\"]).sents)\n",
    "        story[\"sents\"] = [str(s) for s in story[\"sents\"]]\n",
    "\n",
    "    return stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "684a90ab-7f82-4f10-8239-b2102d4adac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(input_list: list[Any], max_item_count: int) -> list[list[Any]]:\n",
    "    \"\"\"\n",
    "    Splits a list of strings into a seperate lists with specified maximum number of items\n",
    "    \"\"\"\n",
    "    return [input_list[i:i+max_item_count] for i in range(0, len(input_list), max_item_count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0433624d-f681-4ff1-8bfa-9c90895446f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_sentences(stories: list[dict], chunk_size: int = 10) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Breaks the list of sentences into sublists with a maximum item count of chunk_size\n",
    "    \n",
    "    **Note**: This is an inplace operation\n",
    "    \"\"\"\n",
    "    logger.debug(f\"Chunking sentences, chunk_size: {chunk_size}\")\n",
    "    \n",
    "    if \"sents\" not in stories[0].keys():\n",
    "        raise ValueError(\"Out of order operation: Cannot chunk sentences before they exist\")\n",
    "        \n",
    "    for story in log_tqdm(stories, \"Chunking\"):\n",
    "        story[\"sent_chunks\"] = split_list(story[\"sents\"], chunk_size)\n",
    "        \n",
    "    return stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c210858e-cb3d-4e6d-8c5e-b928a15132da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_length_metadata(stories: list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Collects minor metadata regarding the length of the article\n",
    "    \n",
    "    **Note**: This is an inplace operation\n",
    "    \"\"\"\n",
    "    logger.debug(\"Adding article length metadata\")\n",
    "    \n",
    "    if any(key not in stories[0] for key in [\"sents\", \"sent_chunks\"]):\n",
    "        raise ValueError(\"Out of order operation: Collecting metadata requires all sub-items to be populated\")\n",
    "        \n",
    "    for story in log_tqdm(stories, \"Collecting Metadata\"):\n",
    "        story[\"num_sents\"] = len(story[\"sents\"])\n",
    "        story[\"num_tokens\"] = len(story[\"text\"]) // 4\n",
    "        story[\"num_words\"] = len(story[\"text\"].split(\" \"))\n",
    "        story[\"num_chars\"] = len(story[\"text\"])\n",
    "        story[\"num_chunks\"] = len(story[\"sent_chunks\"])\n",
    "\n",
    "    return stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a4d7288-d60a-4c08-8540-9a53d934b7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_into_chunk_list(story_list: list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Seperates the chunks in the story dict into individual dicts and returns\n",
    "    a list with each of these chunks as their own item\n",
    "    \"\"\"\n",
    "    logger.debug(\"Creating list of chunks from sublist in article dict\")\n",
    "    \n",
    "    chunk_list = []\n",
    "    for item in log_tqdm(story_list, \"Seperating Chunks\"):\n",
    "        # print(item[\"sent_chunks\"])\n",
    "        for chunk in item[\"sent_chunks\"]:\n",
    "\n",
    "            # populate each chunk with articles original metadata\n",
    "            chunk_dict = {\n",
    "                \"title\": item[\"title\"],\n",
    "                \"authors\": item[\"authors\"],\n",
    "                \"date\": item[\"date\"],\n",
    "                \"source\": item[\"source\"],\n",
    "                \"url\": item[\"url\"],\n",
    "            }\n",
    "    \n",
    "            # rejoin chunk sentences and format to more natural text (i.e., format end of sentence text)\n",
    "            chunk_text = \"\".join(chunk).replace(\"  \", \" \").strip()\n",
    "            chunk_text = re.sub(r\"\\.([A-Z])\", r\". \\1\", chunk_text)    \n",
    "            chunk_dict[\"text\"] = chunk_text\n",
    "\n",
    "            # don't log here because it would get messy\n",
    "            chunk_list.append(chunk_dict)\n",
    "\n",
    "    return chunk_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e0ba8d9-8d6c-4d3f-97f2-aba8fee1a642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_length_metadata(chunks: list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Collects metadata regarding the length of the chunks\n",
    "    \n",
    "    **Note**: This is an inplace operation\n",
    "    \"\"\"\n",
    "    logger.debug(\"Adding article length metadata\")\n",
    "            \n",
    "    for chunk in log_tqdm(chunks, \"Collecting Metadata\"):\n",
    "        chunk[\"num_tokens\"] = len(chunk[\"text\"]) // 4\n",
    "        chunk[\"num_words\"] = len(chunk[\"text\"].split(\" \"))\n",
    "        chunk[\"num_chars\"] = len(chunk[\"text\"])\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e40990d7-fe38-4f46-ae5c-945d9e13ae2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_articles(stories: list[dict], sentences_per_chunk: int = 10) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Converts the list of stories (stored as dicts) into a list chunks (also dicts)\n",
    "\n",
    "    sentences_per_chunks sets the number of sentences used in each chunk\n",
    "    \"\"\"\n",
    "    logger.debug(\"Chunking list of articles\")\n",
    "\n",
    "    # create chunk information within a copy of the stories list[dict]\n",
    "    story_copy = copy_list_of_dicts(stories)\n",
    "    story_copy = sentencize_stories(story_copy)\n",
    "    story_copy = chunk_sentences(story_copy)\n",
    "\n",
    "    # create a new chunk list[dict] from result\n",
    "    chunks = seperate_into_chunk_list(story_copy)\n",
    "    chunks = chunk_length_metadata(chunks)\n",
    "    \n",
    "    return chunks    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be4723d-91bf-4ab7-9d90-1cd61f6a8561",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Create embeddings from chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1cfe5a19-e518-4487-9282-597a5de80543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_embeddings(\n",
    "    chunk_list: list[dict], \n",
    "    embedding_model: SentenceTransformer\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Uses the embedded model to create embeddings from the text values in the provided chunks\n",
    "\n",
    "    **Note**: this operation is inplace\n",
    "    \"\"\"\n",
    "    logger.debug(\"Creating embeddings for chunks\")\n",
    "\n",
    "    chunk_texts = [chunk[\"text\"] for chunk in chunk_list]\n",
    "    chunk_embeddings = embedding_model.encode(\n",
    "        chunk_texts, \n",
    "        batch_size=32, \n",
    "        convert_to_tensor=True, \n",
    "        show_progress_bar=logger.isEnabledFor(logging.DEBUG)\n",
    "    )\n",
    "\n",
    "    for chunk, embedding in zip(chunk_list, chunk_embeddings):\n",
    "        chunk[\"embedding\"] = embedding\n",
    "\n",
    "    return chunk_list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0e4cd8d3-e9b4-4236-8171-2c57e6d56fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_and_embed(\n",
    "    articles_list: list[dict], \n",
    "    embedding_model: SentenceTransformer | None = None,\n",
    ") -> list[dict]:\n",
    "\n",
    "    # initialize a model if one was not provided\n",
    "    embedding_model = embedding_model or EMBEDDING_MODEL\n",
    "\n",
    "    # create chunks and add embeddings\n",
    "    chunks = chunk_articles(articles_list)\n",
    "    chunks = add_embeddings(chunks, embedding_model=embedding_model)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e8bbd2-5153-4cf4-9a09-802717362910",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Storage in QDrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6050b31c-5391-467f-bc01-173ad19b1268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_chunk_list_to_db(chunk_list: dict):\n",
    "    \"\"\"Adds a single chunk to the vector database\"\"\"\n",
    "    logger.debug(f\"Adding chunks to vector DB, collection: {COLLECTION_NAME}\")\n",
    "\n",
    "    points = [\n",
    "        PointStruct(\n",
    "            id = str(uuid4()),\n",
    "            vector = chunk[\"embedding\"],\n",
    "            payload = {\n",
    "                \"source\": chunk[\"source\"],\n",
    "                \"date\": chunk[\"date\"],\n",
    "                \"url\": chunk[\"url\"],\n",
    "                \"title\": chunk[\"title\"],\n",
    "                \"authors\": chunk[\"authors\"],\n",
    "                \"text\": chunk[\"text\"],\n",
    "            }\n",
    "        )\n",
    "        for chunk in chunk_list\n",
    "    ]\n",
    "\n",
    "    VECTOR_DB.upsert(collection_name=COLLECTION_NAME, points=points)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "bd11c64a-fc6d-46d8-9ce4-35cb0c0cd6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks = chunk_and_embed(test_stories)\n",
    "# add_chunk_list_to_db(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2a6a75-c764-430d-94e2-06ad9dbf5a08",
   "metadata": {},
   "source": [
    "## Qdrant Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "7adc1a80-c31f-46d7-9c22-d84d5e125cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_response_to_dict(\n",
    "    response: QueryResponse, \n",
    "    embedding_model: SentenceTransformer\n",
    ") -> list[dict]:\n",
    "    \"\"\"Converts the output of the qdrant similarity search to a list of dicts\"\"\"\n",
    "    logger.debug(\"Converting query results into a dictionary\")\n",
    "\n",
    "    results = []\n",
    "    for point in response.points:\n",
    "        new_dict = point.payload.copy()\n",
    "        new_dict[\"score\"] = point.score\n",
    "        new_dict[\"vector\"] = point.vector\n",
    "        results.append(new_dict)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "a85d3d98-228b-4af2-a221-7e22fd129823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_db(\n",
    "    query_str: str, embedding_model: SentenceTransformer, num_retrieve: int = 5\n",
    ") -> QueryResponse:\n",
    "    \"\"\"Method for searching for qdrant db for similar chunks to provided quote\"\"\"\n",
    "    \n",
    "    # leave embeddings in default type. Tensor isn't accepted and converting to np increases runtime.\n",
    "    query_embeddings = embedding_model.encode(query_str)\n",
    "    results = VECTOR_DB.query_points(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        query=query_embeddings,\n",
    "        limit=num_retrieve,\n",
    "        with_payload=True,\n",
    "        with_vectors=True\n",
    "    )\n",
    "    \n",
    "    return query_response_to_dict(results, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "be72b3a3-1fa8-469a-8d6b-89e32b7ecbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_response(\n",
    "    query_str: str,\n",
    "    resp_list: list[dict],\n",
    "    reranker_model: CrossEncoder\n",
    ") -> list[tuple[float, dict]]:\n",
    "    \"\"\"\n",
    "    Takes the output from our search in the vector database and reranks based on crossencoder similarity\n",
    "    \"\"\"\n",
    "    logger.debug(f\"Reranking responses [num_query_results: {len(resp_list)}]\")\n",
    "\n",
    "    # extract the original chunk text from the payloads\n",
    "    response_texts = [item[\"text\"] for item in resp_list]\n",
    "\n",
    "    # pass query string with text into reranker\n",
    "    text_pairs = [(query_str, text) for text in response_texts]\n",
    "    new_scores = reranker_model.predict(\n",
    "        text_pairs, \n",
    "        batch_size=32, \n",
    "        show_progress_bar=logger.isEnabledFor(logging.DEBUG),\n",
    "    )\n",
    "\n",
    "    ordered_results = sorted(zip(new_scores.astype(float), resp_list), key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    return ordered_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "ec14d79f-89c4-43a5-801d-25c1589d5952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_similar(\n",
    "    query_str: str, \n",
    "    embedding_model: SentenceTransformer | None = None,\n",
    "    reranker_model: CrossEncoder | None = None,\n",
    "    num_retrieve: int = 5,\n",
    ") -> list[tuple[float, dict]]:\n",
    "    \"\"\"Retrieves the most similar items in the db to the query\"\"\"\n",
    "    logger.debug(f\"Querying VECTOR_DB for chunks similar to: {query_str} and rank\")\n",
    "\n",
    "    # if no embedding model sets to globally initialize one\n",
    "    embedding_model = embedding_model or EMBEDDING_MODEL\n",
    "    reranker_model = reranker_model or RERANKER_MODEL\n",
    "\n",
    "    # search for 3 times the requested amount and filter via the re-ranker\n",
    "    search_results = query_db(query_str, EMBEDDING_MODEL, num_retrieve * 3)\n",
    "    rerank = rerank_response(query_str, search_results, RERANKER_MODEL)\n",
    "    \n",
    "    return rerank[:num_retrieve]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "23ab223d-be7f-4a99-96a2-20ee0de08da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 00:03:01,179 - DEBUG - Querying VECTOR_DB for chunks similar to: Trump immigration and rank\n",
      "2025-08-04 00:03:01,215 - DEBUG - Converting query results into a dictionary\n",
      "2025-08-04 00:03:01,215 - DEBUG - Reranking responses [num_query_results: 30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b55049971a974d998a68e8aeeca361de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = retrieve_similar(\"Trump immigration\", num_retrieve=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c63442e-8a04-4ada-8424-532880ab6479",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0][\"title\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bb323a-762e-4152-b200-ba7ac2ab69ef",
   "metadata": {},
   "source": [
    "## Ranking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4a8f2a-1de5-444d-8de3-d2a5c06ebd98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3eaf3c0-ad13-473f-9e5b-66ce82bbea79",
   "metadata": {},
   "source": [
    "## Configuring a local LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b77154-c675-45dd-9506-dd958ae38707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15a7fc74-7a0c-4f67-bf90-b60150667971",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "- Local Retrieval Augmented Generation (RAG) from Scratch (step by step tutorial) by Daniel Bourke\n",
    "\n",
    "link: https://www.youtube.com/watch?v=qN_2fnOPY-M"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
