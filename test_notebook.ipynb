{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7520324a-b451-429e-8aa3-92f38a026093",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4857e506-4a7d-47af-aa70-e76ee1274119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import re\n",
    "import logging\n",
    "from enum import Enum\n",
    "from typing import Any\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Third-party libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "import newspaper as news\n",
    "from tqdm.notebook  import tqdm\n",
    "from newspaper.mthreading import fetch_news\n",
    "from gdeltdoc import GdeltDoc, Filters\n",
    "from spacy.lang.en import English\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb95691f-441a-4f3e-88b3-e046a210fec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b31a041-8598-45ae-8c71-798d5443da4d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Extract data from GDelt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2851c793-fb06-4e2d-805e-4c480efcb6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc34884a-55ae-41bf-a8c9-fd2fdbeaa13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_tqdm(iterable, desc=None, ignore: bool = False):\n",
    "    \"\"\"Only use tqdm progress bar while in debugging\"\"\"\n",
    "    if not ignore and logger.isEnabledFor(logging.DEBUG):\n",
    "        return tqdm(iterable, desc=desc)\n",
    "\n",
    "    return iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e903a91a-1c79-476d-9aca-e38e356bcaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Source(Enum):\n",
    "    # CBC = \"cbc.ca\"\n",
    "    CNN = \"cnn.com\"\n",
    "    # AP = \"apnews.com\"    # wasn't working for some reason\n",
    "    BBC = \"bbc.co.uk\"\n",
    "    # Wired = \"wired.com\"\n",
    "    # Reuters = \"reuters.com\"\n",
    "    NyTimes = \"nytimes.com\"\n",
    "    Guardian = \"theguardian.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4ac50cf-a0db-49c7-8032-8fa460816022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_text(text: str) -> str:\n",
    "    \"\"\"Removes newline characters and leading/trailing whitespace\"\"\"\n",
    "    return text.replace(\"\\n\", \" \").strip()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d940936-fed6-4b39-92b9-94a30ac206fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gdelt_row_to_dict(gd_row: pd.Series) -> dict:\n",
    "    \"\"\"Helper method to convert the output of a gd.article_search row into a dict\"\"\"\n",
    "    return {\n",
    "        \"url\": gd_row[\"url\"],\n",
    "        \"title\": gd_row[\"title\"],\n",
    "        \"domain\": gd_row[\"domain\"],\n",
    "        \"country\": gd_row[\"sourcecountry\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00151388-2232-4f13-99e8-bae51e588351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_to_dict(art: news.Article) -> dict:\n",
    "    \"\"\"Helper method to convert a Article into a dict\"\"\"\n",
    "    return {\n",
    "        \"text\": format_text(art.text),\n",
    "        \"title\": art.title,\n",
    "        \"authors\": art.authors,\n",
    "        \"date\": art.publish_date,\n",
    "        \"source\": art.source_url,\n",
    "        \"url\": art.original_url\n",
    "    }    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a9f5cc9-0f9a-4a84-9ae3-982fde4fb395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gdelt_stories(\n",
    "    keywords: str | None = None,\n",
    "    theme: str | None = None,\n",
    "    sources: list[Source] | None = None,\n",
    "    start_date: datetime = datetime.now().date() - timedelta(days=7),\n",
    "    end_date: datetime = datetime.now().date(),\n",
    "    num_records: int = 5\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Retrieves metadata and urls for relevant stories via the GDelt API\n",
    "    \"\"\"\n",
    "    \n",
    "    if keywords and len(keywords) < 5:\n",
    "        raise ValueError(\"Keywords must be gte 5 characters or the Gdelt API errors.\")\n",
    "\n",
    "    sources = sources or [s for s in Source]        \n",
    "    sources = [s.value for s in sources]\n",
    "        \n",
    "    if len(sources) < 2:\n",
    "        raise ValueError(\"Number of sources must be gte 2 or the Gdelt API errors.\")\n",
    "\n",
    "    if start_date > datetime.now().date() or end_date > datetime.now().date():\n",
    "        raise ValueError(\"News cannot be in the future...\")\n",
    "\n",
    "    if start_date > end_date:\n",
    "        raise ValueError(\"How you gunna end before you start?!?!\")        \n",
    "     \n",
    "    sources = [s for s in sources]\n",
    "    \n",
    "    start_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "    end_str = end_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    if keywords and theme:\n",
    "        f = Filters(\n",
    "            start_date = start_str,\n",
    "            end_date = end_str,\n",
    "            num_records = num_records,\n",
    "            domain = sources,\n",
    "            country = [\"UK\", \"US\"],\n",
    "            language = \"eng\",\n",
    "            keyword = keywords,\n",
    "            theme=theme\n",
    "        )\n",
    "    elif keywords:\n",
    "        f = Filters(\n",
    "            start_date = start_str,\n",
    "            end_date = end_str,\n",
    "            num_records = num_records,\n",
    "            domain = sources,\n",
    "            country = [\"US\"],#, \"UK\"],\n",
    "            language = \"eng\",\n",
    "            keyword = keywords,\n",
    "        )\n",
    "    elif theme:\n",
    "        f = Filters(\n",
    "            start_date = start_str,\n",
    "            end_date = end_str,\n",
    "            num_records = num_records,\n",
    "            domain = sources,\n",
    "            country = [\"UK\", \"US\"],\n",
    "            language = \"eng\",\n",
    "            theme=theme\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"both theme and keywords cannot be empty\")\n",
    "        \n",
    "    gd = GdeltDoc()\n",
    "    search_results = gd.article_search(f)\n",
    "\n",
    "    results = []\n",
    "    for i, row in search_results.iterrows():\n",
    "        results.append(gdelt_row_to_dict(row))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18ce12a4-7e8f-4d68-a0e2-838e9afc7026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles(urls: list[str] | str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Queries newspaper4k for the article associated with the provided url\n",
    "\n",
    "    Note:   fetch_news is an alternative to mutlithread the retrieval process.\n",
    "            However, this does not include the text property sooo were stuck with this.\n",
    "    \"\"\"\n",
    "\n",
    "    urls = urls if isinstance(urls, list) else [urls]\n",
    "    results = []\n",
    "    view_titles = set()\n",
    "    for url in log_tqdm(urls, \"Collecting Stories\"):\n",
    "\n",
    "        # news.articles is an expensive operation so verify \n",
    "        # the current story hasn't already been retrieved\n",
    "        possible_title = url.rsplit('/', 1)[-1]\n",
    "        if possible_title in view_titles:\n",
    "            logger.debug(f\"Removed duplicate story: {possible_title}\")\n",
    "            continue\n",
    "        view_titles.add(possible_title)\n",
    "        \n",
    "        try:\n",
    "            art = news.article(url)         \n",
    "        except:\n",
    "            # in case an article doesn't exist (404 returned) or can't be accessed \n",
    "            continue\n",
    "        \n",
    "        results.append(article_to_dict(art))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48f9809c-1561-4227-9957-2396512918b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_news(\n",
    "    keywords: str | None = None,\n",
    "    theme: str | None = None,\n",
    "    sources: list[Source] | None = None,\n",
    "    start_date: datetime = datetime.now().date() - timedelta(days=7),\n",
    "    end_date: datetime = datetime.now().date(),\n",
    "    num_stories: int = 5,\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Retrieves stories related to the specified keywords\n",
    "\n",
    "    **NOTE**: theme must be a value from the available list here: \n",
    "    http://data.gdeltproject.org/api/v2/guides/LOOKUP-GKGTHEMES.TXT\n",
    "    \"\"\"\n",
    "\n",
    "    logger.debug(\"Gdelt: retrieval beginning\")\n",
    "    story_links = gdelt_stories(\n",
    "        keywords=keywords, \n",
    "        theme=theme,\n",
    "        sources=sources, \n",
    "        start_date=start_date, \n",
    "        end_date=end_date, \n",
    "        num_records=num_stories\n",
    "    )\n",
    "\n",
    "    logger.debug(\"newpaper4k: retrieval beginning\")\n",
    "    stories = get_articles(\n",
    "        urls=[story[\"url\"] for story in story_links]\n",
    "    )\n",
    "\n",
    "    return stories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ff85f2-b1cb-42d1-84e6-d58f9987476c",
   "metadata": {},
   "source": [
    "#### Example for retrieving stories:\n",
    "\n",
    "\\*\\*Note\\*\\*: retrieve_news returns a list of dicts in the format created by the article_to_dict function\n",
    "\n",
    "(i.e., keys=[title, text, domain, country, author, url, date])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cffef02-c75c-4b3c-91ac-cbe3dba077da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stories = retrieve_news(theme=\"IMMIGRATION\", num_stories=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95ce9883-28cc-47fe-8548-ae6879eba743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stories[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f652b316-4555-4709-be11-a26e439ff13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [u[\"url\"] for u in stories]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a79ff7d-12c8-47fe-9091-7f7904ec9f63",
   "metadata": {},
   "source": [
    "Results from above:\n",
    "\n",
    "dict_keys(['text', 'title', 'authors', 'date', 'source', 'url'])\n",
    "\n",
    "['https://www.theguardian.com/us-news/2025/jul/29/states-sue-trump-administration-snap-recipients-data',\n",
    " 'https://www.bbc.co.uk/news/articles/clyjggjplyqo',\n",
    " 'https://www.theguardian.com/us-news/2025/jul/30/ice-hiring-incentives-signing-bonuses',\n",
    " 'https://www.cnn.com/2025/07/30/politics/immigration-employees-reader-callout',\n",
    " 'https://www.theguardian.com/us-news/2025/jul/28/trump-acknowledges-real-starvation-in-gaza-and-tells-israel-to-let-in-every-ounce-of-food',\n",
    " 'https://www.theguardian.com/us-news/2025/aug/01/judge-tps-temporary-protected-status-trump-deportation',\n",
    " 'https://www.theguardian.com/society/2025/jul/30/population-migration-england-wales-data',\n",
    " 'https://www.theguardian.com/world/2025/jul/30/mexico-sheinbaum-alligator-alcatraz-trump',\n",
    " 'https://www.theguardian.com/uk-news/2025/aug/01/social-media-ads-promoting-small-boat-crossings-uk-banned']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b75b88e-7c36-448e-97a9-8089a861291e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Test samples for creating article embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a8bc311-8190-4dae-bba1-f8de91e7a4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 05:32:04,583 - DEBUG - Gdelt: retrieval beginning\n",
      "2025-08-03 05:32:05,376 - DEBUG - newpaper4k: retrieval beginning\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f7df1bd968c44609d5eed4ca4277e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting Stories:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 05:32:32,523 - DEBUG - Removed duplicate story: immigration-birthright-citizenship-us-dg\n",
      "2025-08-03 05:32:33,146 - DEBUG - Removed duplicate story: immigration-employees-reader-callout\n",
      "2025-08-03 05:32:34,996 - DEBUG - Removed duplicate story: ice-arrests-migrants-courthouse\n",
      "2025-08-03 05:32:38,090 - DEBUG - Removed duplicate story: deportations-backfiring-trump-analysis\n",
      "2025-08-03 05:32:42,402 - DEBUG - Removed duplicate story: guatemalan-migrant-deported-mexico-trump-administration-return\n",
      "2025-08-03 05:32:42,418 - DEBUG - Removed duplicate story: guatemalan-migrant-deported-mexico-trump-administration-return\n",
      "2025-08-03 05:32:43,230 - DEBUG - Removed duplicate story: sanctuary-immigration-policies-chicago-illinois-lawsuit-dismissed\n"
     ]
    }
   ],
   "source": [
    "test_stories = retrieve_news(\n",
    "    theme=\"IMMIGRATION\", \n",
    "    num_stories=100, \n",
    "    start_date= datetime.now().date() - timedelta(days=90),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bdf346-fc3a-49bb-890f-8e3698a4dda5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Chunk the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "532e9146-6d81-4ab7-aff5-10d36b3db887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_list_of_dicts(dict_list: list[dict]) -> list[dict]:\n",
    "    \"\"\"Helper method to deep copy a list of dicts (not-recursively)\"\"\"\n",
    "    logger.debug(\"Creating copy of target list of dicts\")\n",
    "    new_list = []\n",
    "    for item in dict_list:\n",
    "        new_list.append(item.copy())\n",
    "\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "baaf1e8c-6e92-4cfa-b8ad-b52ec0419c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentencize_stories(stories: list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Uses spacy to convert the block of text provided by newspaper4k into sentences\n",
    "\n",
    "    **Note**: This is an inplace operation\n",
    "    \"\"\"\n",
    "    nlp = English()\n",
    "    _ = nlp.add_pipe(\"sentencizer\")\n",
    "    \n",
    "    logger.debug(\"Breaking text into sentences\")\n",
    "    # convert to sentences and ensure dtype is a str not spacy specific typing\n",
    "    for story in log_tqdm(stories, \"Sentencizing\"):\n",
    "        story[\"sents\"] = list(nlp(story[\"text\"]).sents)\n",
    "        story[\"sents\"] = [str(s) for s in story[\"sents\"]]\n",
    "\n",
    "    return stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "684a90ab-7f82-4f10-8239-b2102d4adac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(input_list: list[Any], max_item_count: int) -> list[list[Any]]:\n",
    "    \"\"\"\n",
    "    Splits a list of strings into a seperate lists with specified maximum number of items\n",
    "    \"\"\"\n",
    "    return [input_list[i:i+max_item_count] for i in range(0, len(input_list), max_item_count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0433624d-f681-4ff1-8bfa-9c90895446f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_sentences(stories: list[dict], chunk_size: int = 10) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Breaks the list of sentences into sublists with a maximum item count of chunk_size\n",
    "    \n",
    "    **Note**: This is an inplace operation\n",
    "    \"\"\"\n",
    "    logger.debug(f\"Chunking sentences, chunk_size: {chunk_size}\")\n",
    "    \n",
    "    if \"sents\" not in stories[0].keys():\n",
    "        raise ValueError(\"Out of order operation: Cannot chunk sentences before they exist\")\n",
    "        \n",
    "    for story in log_tqdm(stories, \"Chunking\"):\n",
    "        story[\"sent_chunks\"] = split_list(story[\"sents\"], chunk_size)\n",
    "        \n",
    "    return stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c210858e-cb3d-4e6d-8c5e-b928a15132da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_length_metadata(stories: list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Collects minor metadata regarding the length of the article\n",
    "    \n",
    "    **Note**: This is an inplace operation\n",
    "    \"\"\"\n",
    "    logger.debug(\"Adding article length metadata\")\n",
    "    \n",
    "    if any(key not in stories[0] for key in [\"sents\", \"sent_chunks\"]):\n",
    "        raise ValueError(\"Out of order operation: Collecting metadata requires all sub-items to be populated\")\n",
    "        \n",
    "    for story in log_tqdm(stories, \"Collecting Metadata\"):\n",
    "        story[\"num_sents\"] = len(story[\"sents\"])\n",
    "        story[\"num_tokens\"] = len(story[\"text\"]) // 4\n",
    "        story[\"num_words\"] = len(story[\"text\"].split(\" \"))\n",
    "        story[\"num_chars\"] = len(story[\"text\"])\n",
    "        story[\"num_chunks\"] = len(story[\"sent_chunks\"])\n",
    "\n",
    "    return stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a4d7288-d60a-4c08-8540-9a53d934b7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_into_chunk_list(story_list: list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Seperates the chunks in the story dict into individual dicts and returns\n",
    "    a list with each of these chunks as their own item\n",
    "    \"\"\"\n",
    "    logger.debug(\"Creating list of chunks from sublist in article dict\")\n",
    "    \n",
    "    chunk_list = []\n",
    "    for item in log_tqdm(story_list, \"Seperating Chunks\"):\n",
    "        # print(item[\"sent_chunks\"])\n",
    "        for chunk in item[\"sent_chunks\"]:\n",
    "\n",
    "            # populate each chunk with articles original metadata\n",
    "            chunk_dict = {\n",
    "                \"title\": item[\"title\"],\n",
    "                \"authors\": item[\"authors\"],\n",
    "                \"date\": item[\"date\"],\n",
    "                \"source\": item[\"source\"],\n",
    "                \"url\": item[\"url\"],\n",
    "            }\n",
    "    \n",
    "            # rejoin chunk sentences and format to more natural text (i.e., format end of sentence text)\n",
    "            chunk_text = \"\".join(chunk).replace(\"  \", \" \").strip()\n",
    "            chunk_text = re.sub(r\"\\.([A-Z])\", r\". \\1\", chunk_text)    \n",
    "            chunk_dict[\"text\"] = chunk_text\n",
    "\n",
    "            # don't log here because it would get messy\n",
    "            chunk_list.append(chunk_dict)\n",
    "\n",
    "    return chunk_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e0ba8d9-8d6c-4d3f-97f2-aba8fee1a642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_length_metadata(chunks: list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Collects metadata regarding the length of the chunks\n",
    "    \n",
    "    **Note**: This is an inplace operation\n",
    "    \"\"\"\n",
    "    logger.debug(\"Adding article length metadata\")\n",
    "            \n",
    "    for chunk in log_tqdm(chunks, \"Collecting Metadata\"):\n",
    "        chunk[\"num_tokens\"] = len(chunk[\"text\"]) // 4\n",
    "        chunk[\"num_words\"] = len(chunk[\"text\"].split(\" \"))\n",
    "        chunk[\"num_chars\"] = len(chunk[\"text\"])\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e40990d7-fe38-4f46-ae5c-945d9e13ae2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_articles(stories: list[dict], sentences_per_chunk: int = 10) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Converts the list of stories (stored as dicts) into a list chunks (also dicts)\n",
    "\n",
    "    sentences_per_chunks sets the number of sentences used in each chunk\n",
    "    \"\"\"\n",
    "    logger.debug(\"Chunking list of articles\")\n",
    "\n",
    "    # create chunk information within a copy of the stories list[dict]\n",
    "    story_copy = copy_list_of_dicts(stories)\n",
    "    story_copy = sentencize_stories(story_copy)\n",
    "    story_copy = chunk_sentences(story_copy)\n",
    "\n",
    "    # create a new chunk list[dict] from result\n",
    "    chunks = seperate_into_chunk_list(story_copy)\n",
    "    chunks = chunk_length_metadata(chunks)\n",
    "    \n",
    "    return chunks    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be4723d-91bf-4ab7-9d90-1cd61f6a8561",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Create embeddings from chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4bbb6f43-b742-4549-9db8-024c680cda48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_model(model_name: str = \"all-MiniLM-L6-v2\") -> SentenceTransformer:\n",
    "    \"\"\"Helper method to create the model and send it to cuda\"\"\"\n",
    "    logger.debug(\"Creating embedding model\")\n",
    "    \n",
    "    model = SentenceTransformer(model_name_or_path=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.to(\"cuda\")\n",
    "        logger.debug(f\"CUDA detected. Embedding model moved to {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "    else:\n",
    "        logger.debug(\"CUDA not detected. Embedding model will remain on CPU\")\n",
    "\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1cfe5a19-e518-4487-9282-597a5de80543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding(\n",
    "    chunks: list[dict], \n",
    "    embedding_model: SentenceTransformer\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Uses the embedded model to create embeddings from the text values in the provided chunks\n",
    "\n",
    "    **Note**: this operation is inplace\n",
    "    \"\"\"\n",
    "    logger.debug(\"Creating embeddings for chunks\")\n",
    "\n",
    "    for chunk in log_tqdm(chunks, \"Embedding\"):\n",
    "        chunk[\"embedding\"] = embedding_model.encode(chunk[\"text\"])\n",
    "\n",
    "    return chunks        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b239928b-604c-48bb-983a-ed813c6264a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 05:56:33,096 - DEBUG - Chunking list of articles\n",
      "2025-08-03 05:56:33,096 - DEBUG - Creating copy of target list of dicts\n",
      "2025-08-03 05:56:33,132 - DEBUG - Breaking text into sentences\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b027ce6a6e04cd594768a451f8b0f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sentencizing:   0%|          | 0/93 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 05:56:33,397 - DEBUG - Chunking sentences, chunk_size: 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d8ce3c909934e45bfa8ae1d51d3646b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunking:   0%|          | 0/93 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 05:56:33,412 - DEBUG - Creating list of chunks from sublist in article dict\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "830402bc0ee74b5aa6a7e0cb53375c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Seperating Chunks:   0%|          | 0/93 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 05:56:33,412 - DEBUG - Adding article length metadata\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd68ac57a4d9436f8888cef359c51c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting Metadata:   0%|          | 0/396 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 05:56:33,412 - DEBUG - Creating embedding model\n",
      "2025-08-03 05:56:34,448 - DEBUG - CUDA detected. Embedding model moved to NVIDIA GeForce RTX 5070 Ti\n",
      "2025-08-03 05:56:34,448 - DEBUG - Creating embeddings for chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df14cdfbf041406cb273682726dc55cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Embedding:   0%|          | 0/396 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jacob\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "chunks = chunk_articles(test_stories)\n",
    "\n",
    "embedding_model = create_embedding_model()\n",
    "chunks = create_embedding(chunks, embedding_model=embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e8bbd2-5153-4cf4-9a09-802717362910",
   "metadata": {},
   "source": [
    "## Data Storage in QDrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a9bab84-6d01-4d88-9a59-96aaa9108165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - title : The least ‘integrated’ part of British society isn’t the immigrants – it’s the elite | Andy Beckett\n",
      " - authors : ['Andy Beckett', 'www.theguardian.com']\n",
      " - date : 2025-05-23 00:00:00\n",
      " - source : https://www.theguardian.com\n",
      " - url : https://www.theguardian.com/commentisfree/2025/may/23/integrated-british-society-immigrants-elite\n",
      " - text : Inside, to my surprise, was a classic Portuguese bar, with dusty Portuguese football scarves hanging from the ceiling and elderly Portuguese immigrants drinking dark Portuguese liqueur from tiny glasses. The bar felt both foreign and, in its confident approach to cultural difference, quite British. Were the bar staff and their customers completely integrated with the rest of King’s Lynn?On the basis of a brief visit, it was hard to say. But they made a very good cup of tea.\n",
      " - num_tokens : 119\n",
      " - num_words : 79\n",
      " - num_chars : 478\n"
     ]
    }
   ],
   "source": [
    "for item in chunks:\n",
    "    if item[\"num_chars\"] < 1000:\n",
    "        for k, v in item.items():\n",
    "            print(\" -\", k, \":\", v)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a7fc74-7a0c-4f67-bf90-b60150667971",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "- Local Retrieval Augmented Generation (RAG) from Scratch (step by step tutorial) by Daniel Bourke\n",
    "\n",
    "link: https://www.youtube.com/watch?v=qN_2fnOPY-M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "17ef4904-f150-4db2-9f75-535510fcd0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory:     17094.48 MB\n",
      "Reserved memory:  304.09 MB\n",
      "Allocated memory: 190.90 MB\n",
      "Free within reserved: 113.19 MB\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    gpu_id = torch.cuda.current_device()\n",
    "    total = torch.cuda.get_device_properties(gpu_id).total_memory\n",
    "    reserved = torch.cuda.memory_reserved(gpu_id)\n",
    "    allocated = torch.cuda.memory_allocated(gpu_id)\n",
    "    free = reserved - allocated\n",
    "\n",
    "    print(f\"Total memory:     {total / 1e6:.2f} MB\")\n",
    "    print(f\"Reserved memory:  {reserved / 1e6:.2f} MB\")\n",
    "    print(f\"Allocated memory: {allocated / 1e6:.2f} MB\")\n",
    "    print(f\"Free within reserved: {free / 1e6:.2f} MB\")\n",
    "else:\n",
    "    print(\"No CUDA device available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3dabc5-0edf-4c96-9964-3ec9ed99ccd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
